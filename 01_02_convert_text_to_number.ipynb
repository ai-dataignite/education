{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 원핫 인코딩 직접 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = 'you know I want your love. because I love you.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TreebankWordTokenizer 를 통해서 토큰화\n",
    "from nltk.tokenize import word_tokenize\n",
    "token = word_tokenize(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'you': 0, 'know': 1, 'I': 2, 'want': 3, 'your': 4, 'love': 5, '.': 6, 'because': 7}\n"
     ]
    }
   ],
   "source": [
    "# 어휘사전 (vocabulary) 생성\n",
    "word2index={}\n",
    "for voca in token:\n",
    "    if voca not in word2index.keys():\n",
    "        word2index[voca]=len(word2index)\n",
    "print(word2index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one-out-encoding 구현 함수\n",
    "def one_hot_encoding(word, word2index):\n",
    "    one_hot_vector = [0]*(len(word2index)) # 0 으로 초기화된 배열을 만듬\n",
    "    index=word2index[word] # 어휘사전에서 인덱스를 반환받아\n",
    "    one_hot_vector[index]=1 # 해당 인덱스에 1로 업데이트\n",
    "    return one_hot_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 0, 0, 1, 0, 0, 0, 0]"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "one_hot_encoding(\"want\",word2index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### scikit-learn 라이브러리를 이용한 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "ohe = OneHotEncoder(sparse=False)\n",
    "vect = ohe.fit_transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0., 0., 0., 1., 0.],\n",
       "       [0., 0., 0., 1., 0., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 1., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 1.],\n",
       "       [0., 0., 0., 0., 1., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 1., 0., 0., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 1., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 1., 0.],\n",
       "       [1., 0., 0., 0., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['x0_.', 'x0_I', 'x0_because', 'x0_know', 'x0_love', 'x0_want',\n",
       "       'x0_you', 'x0_your'], dtype=object)"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ohe.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bag of words 직접 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'you': 0, 'know': 1, 'I': 2, 'want': 3, 'your': 4, 'love': 5, '.': 6, 'because': 7}\n"
     ]
    }
   ],
   "source": [
    "word2index={} \n",
    "bow=[]  \n",
    "for voca in token:  \n",
    "    if voca not in word2index.keys():  \n",
    "        word2index[voca]=len(word2index)    \n",
    "        bow.insert(len(word2index)-1,1)\n",
    "    else:\n",
    "        index=word2index.get(voca)\n",
    "        bow[index]=bow[index]+1 \n",
    "print(word2index)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 1, 2, 1, 1, 2, 2, 1]"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### scikit-learn 라이브러리를 이용한 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "                strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                tokenizer=None, vocabulary=None)"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "#vect = CountVectorizer(tokenizer = word_tokenize) # tokenizer 선택\n",
    "vect = CountVectorizer()\n",
    "vect.fit([corpus])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "어휘 사전의 크기: 24\n",
      "어휘 사전의 내용:\n",
      " {'you': 20, 'know': 13, 'want': 18, 'your': 22, 'love': 15, 'you know': 21, 'know want': 14, 'want your': 19, 'your love': 23, 'because': 2, 'because love': 3, 'love you': 16, 'family': 5, 'is': 9, 'an': 0, 'important': 7, 'thing': 17, 'family is': 6, 'is an': 10, 'an important': 1, 'important thing': 8, 'it': 11, 'everything': 4, 'it everything': 12}\n"
     ]
    }
   ],
   "source": [
    "print(\"어휘 사전의 크기: {}\".format(len(vect.vocabulary_)))\n",
    "print(\"어휘 사전의 내용:\\n {}\".format(vect.vocabulary_)) # dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "bag_of_words = vect.transform([corpus])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 0)\t1\n",
      "  (0, 1)\t1\n",
      "  (0, 2)\t2\n",
      "  (0, 3)\t1\n",
      "  (0, 4)\t2\n",
      "  (0, 5)\t1\n"
     ]
    }
   ],
   "source": [
    "print(bag_of_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DTM 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [\"you know I want your love.\", \"because I love you.\", \"Family is an important thing.\",\"It's everything.\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "어휘 사전의 내용:\n",
      " {'you': 11, 'know': 7, 'want': 10, 'your': 12, 'love': 8, 'because': 1, 'family': 3, 'is': 5, 'an': 0, 'important': 4, 'thing': 9, 'it': 6, 'everything': 2}\n"
     ]
    }
   ],
   "source": [
    "vect.fit(corpus)\n",
    "print(\"어휘 사전의 내용:\\n {}\".format(vect.vocabulary_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 7)\t1\n",
      "  (0, 8)\t1\n",
      "  (0, 10)\t1\n",
      "  (0, 11)\t1\n",
      "  (0, 12)\t1\n",
      "  (1, 1)\t1\n",
      "  (1, 8)\t1\n",
      "  (1, 11)\t1\n",
      "  (2, 0)\t1\n",
      "  (2, 3)\t1\n",
      "  (2, 4)\t1\n",
      "  (2, 5)\t1\n",
      "  (2, 9)\t1\n",
      "  (3, 2)\t1\n",
      "  (3, 6)\t1\n"
     ]
    }
   ],
   "source": [
    "bag_of_words = vect.transform(corpus)\n",
    "print(bag_of_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### n-gram 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "어휘 사전 크기: 24\n",
      "어휘 사전:\n",
      "['an', 'an important', 'because', 'because love', 'everything', 'family', 'family is', 'important', 'important thing', 'is', 'is an', 'it', 'it everything', 'know', 'know want', 'love', 'love you', 'thing', 'want', 'want your', 'you', 'you know', 'your', 'your love']\n"
     ]
    }
   ],
   "source": [
    "vect = CountVectorizer(ngram_range=(1, 2)).fit(corpus)\n",
    "print(\"어휘 사전 크기: {}\".format(len(cv.vocabulary_)))\n",
    "print(\"어휘 사전:\\n{}\".format(cv.get_feature_names()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 13)\t1\n",
      "  (0, 14)\t1\n",
      "  (0, 15)\t1\n",
      "  (0, 18)\t1\n",
      "  (0, 19)\t1\n",
      "  (0, 20)\t1\n",
      "  (0, 21)\t1\n",
      "  (0, 22)\t1\n",
      "  (0, 23)\t1\n",
      "  (1, 2)\t1\n",
      "  (1, 3)\t1\n",
      "  (1, 15)\t1\n",
      "  (1, 16)\t1\n",
      "  (1, 20)\t1\n",
      "  (2, 0)\t1\n",
      "  (2, 1)\t1\n",
      "  (2, 5)\t1\n",
      "  (2, 6)\t1\n",
      "  (2, 7)\t1\n",
      "  (2, 8)\t1\n",
      "  (2, 9)\t1\n",
      "  (2, 10)\t1\n",
      "  (2, 17)\t1\n",
      "  (3, 4)\t1\n",
      "  (3, 11)\t1\n",
      "  (3, 12)\t1\n"
     ]
    }
   ],
   "source": [
    "bag_of_words = vect.transform(corpus)\n",
    "print(bag_of_words)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "StudyML",
   "language": "python",
   "name": "studyml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
